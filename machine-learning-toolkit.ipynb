{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File in progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Some untidy book examples inside\n",
    "- and missing alghoritms\n",
    "- as well as optimisation techniques\n",
    "- File will be upgraded on daily basis\n",
    "- Quick, easy to use\n",
    "- End to end examples in other folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Machine Learning toolkit\n",
    "\n",
    "\n",
    "\n",
    "#### 1. Fundaments\n",
    "\n",
    "1. Data generators\n",
    "2. Preprocessing data (more in data-preprocessing-toolkit)\n",
    "3. Train/Test split\n",
    "4. Fit model\n",
    "5. Predictions\n",
    "\n",
    "\n",
    "#### 2. Example\n",
    "\n",
    "1. End to end example (more examples in ML-use-cases folder)\n",
    "\n",
    "\n",
    "#### 3. Metrics\n",
    "\n",
    "1. Classification: Accuracy, Recall, Precision, F1, Confusion Matrix, Classification Report\n",
    "2. Regression: MAE, MSE, R2\n",
    "3. Clustering: Adjusted Rand Index, Homogeneity, V-measure\n",
    "\n",
    "\n",
    "#### 4. Machine Learning Alghoritms\n",
    "\n",
    "I. Regression\n",
    "- Linear Regression\n",
    "- Logistic Regression\n",
    "- Stepwise Regression\n",
    "\n",
    "II. Instance-based\n",
    "- KNN - K-Nearest Neighbours\n",
    "\n",
    "III. Support Vectore Machines\n",
    "- SVM - Support Vector Machine\n",
    "\n",
    "IV. Bayesian\n",
    "- NB - Naive Bayes\n",
    "- GBN - Gaussian Naive Bayes\n",
    "- BBN - Bayesian Belief Network\n",
    "\n",
    "V. Regularisation\n",
    "- Ridge Regression\n",
    "- LASSO - Least Actual Shrinkage and Sellection Operator\n",
    "- Elastic Net\n",
    "\n",
    "VI. Rule system\n",
    "- One Rule\n",
    "- Zero Rule\n",
    "- Repeated Incremental Pruning to Produce Error Reduction\n",
    "\n",
    "VII. Decision Trees\n",
    "- CAT - Classification And Regression Trees\n",
    "- CHAID - Chi-squared Automatic Interaction Detection\n",
    "- Conditional Decision Tree\n",
    "\n",
    "VIII. Ensemble\n",
    "- RF - Random Forrest\n",
    "- Gradient Boosted Machines\n",
    "- GBRT - Gradient Boostet Regression Trees\n",
    "\n",
    "IX. Clustering\n",
    "- K-Means\n",
    "- K-Medians\n",
    "- Hierarchical Clustering\n",
    "\n",
    "X. Dimensionallity Reduction\n",
    "- PCA - Principial Component Analysis\n",
    "- LDA - Linear Discriminant Analysis\n",
    "- RDA - Principial Component Regression\n",
    "\n",
    "\n",
    "#### 5. Model tuning\n",
    "\n",
    "1. Grid Search\n",
    "2. Randomized Parameter Optimization\n",
    "3. Cross-Validation\n",
    "\n",
    "\n",
    "#### TBD\n",
    "1. Pipes\n",
    "\n",
    "\n",
    "\n",
    "To be developed further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundaments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Real datasets\n",
    "# -------------\n",
    "df = pd.read_csv('datasets/tweets_prepared.csv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "\n",
    "# Also, example datasets\n",
    "# ----------------------\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generators\n",
    "# ---------------\n",
    "\n",
    "from sklearn.datasets.samples_generator import make_swiss_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data (more in data-preprocessing-toolkit)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# Missing Values\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values=0, strategy='mean', axis=0)\n",
    "imp.fit_transform(X_train)\n",
    "\n",
    "# Standarization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "standardized_X = scaler.transform(X_train)\n",
    "standardized_X_test = scaler.transform(X_test)\n",
    "\n",
    "# Normalization\n",
    "from sklearn.preprocessing import Normalizer\n",
    "scaler = Normalizer().fit(X_train)\n",
    "normalized_X = scaler.transform(X_train)\n",
    "normalized_X_test = scaler.transform(X_test)\n",
    "\n",
    "# Binarization\n",
    "from sklearn.preprocessing import Binarizer\n",
    "binarizer = Binarizer(threshold=0.0).fit(X)\n",
    "binary_X = binarizer.transform(X)\n",
    "\n",
    "# Encoding Categorical Features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "enc = LabelEncoder()\n",
    "y = enc.fit_transform(y)\n",
    "\n",
    "# Generating Polynomial Features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(5)\n",
    "poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test split\n",
    "# ------------------\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, test_size=0.2) # , random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Fitting\n",
    "# -------------\n",
    "\n",
    "# Supervised\n",
    "lr.fit(X, y) # svc, knn (...)\n",
    "\n",
    "# Unsupervised\n",
    "k_means.fit(X_train) # (...)\n",
    "\n",
    "# Dimension Reduction\n",
    "pca_model = pca.fit_transform(X_train) # (...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "# -----------\n",
    "\n",
    "# Predict labels\n",
    "y_pred = lr.predict(X_test) # k_means (...)\n",
    "y_pred = svc.predict(X_test) # SIZE\n",
    "\n",
    "# Predict labels propability\n",
    "y_pred = knn.predict_proba(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import neighbors, datasets, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load exampled dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Pick features and labels/targets\n",
    "X, y = iris.data[:, :2], iris.target\n",
    "\n",
    "# Split features and labels/target into TEST and TRAIN dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Rescale features\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Pick model\n",
    "model = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit data into model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction \n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Pick a metric and check its value\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Further options: visualise, tune, pipe, compare (...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification metrics\n",
    "# ----------------------\n",
    "\n",
    "knn.score(X_test, y_test) # Estimator score method\n",
    "\n",
    "# Accuracy Score\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Recall\n",
    "from sklearn.metrics import recall_score\n",
    "print(recall_score(true_labels, guesses))\n",
    "\n",
    "# Precision score\n",
    "from sklearn.metrics import precision_score\n",
    "print(precision_score(true_labels, guesses))\n",
    "\n",
    "# F1\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(true_labels, guesses))\n",
    "\n",
    "\n",
    "# Regression Metrics\n",
    "# ------------------\n",
    "\n",
    "# Mean Absolute Error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "y_true = [3, -0.5, 2]\n",
    "mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "# Mean Squared Error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# RÂ² Score\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "# Clustering Metrics\n",
    "# ------------------\n",
    "\n",
    "# Adjusted Rand Index\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "adjusted_rand_score(y_true, y_pred)\n",
    "\n",
    "# Homogeneity\n",
    "from sklearn.metrics import homogeneity_score\n",
    "homogeneity_score(y_true, y_pred)\n",
    "\n",
    "# V-measure\n",
    "from sklearn.metrics import v_measure_score\n",
    "metrics.v_measure_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Alghoritms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  I. Regression\n",
    "# --------------\n",
    "# - Linear Regression\n",
    "# - Logistic Regression\n",
    "# - Stepwise Regression\n",
    "\n",
    "\n",
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression(normalize=True)\n",
    "\n",
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_training_data, y_training_data)\n",
    "prediction = model.predict(your_x_data)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    " \n",
    "# Cannot use Rank 1 matrix in scikit learn\n",
    "X = X.reshape((m, 1))\n",
    "# Creating Model\n",
    "reg = LinearRegression()\n",
    "# Fitting training data\n",
    "reg = reg.fit(X, Y)\n",
    "# Y Prediction\n",
    "Y_pred = reg.predict(X)\n",
    " \n",
    "# Calculating R2 Score\n",
    "r2_score = reg.score(X, Y)\n",
    " \n",
    "print(r2_score)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_\n",
    "#(array([ 4.21509616]), array([[ 2.77011339]]))\n",
    "lin_reg.predict(X_new)\n",
    "\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "dataset = pd.read_csv('Social_Network_Ads.csv')\n",
    "X = dataset.iloc[:, [2, 3]].values\n",
    "y = dataset.iloc[:, 4].values\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    " \n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    " \n",
    "print(cm)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred)*100)\n",
    "\n",
    "# Stepwise Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II. Instance-based\n",
    "# ------------------\n",
    "# - KNN - K-Nearest Neighbours\n",
    "\n",
    "\n",
    "# KNN\n",
    "from sklearn.neigbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(x_training_data, y_training_data)\n",
    "predictions = model.predict(your_x_data)\n",
    "probabilities = model.predict_proba(your_x_data)\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(data_train, target_train)\n",
    "pred = neigh.predict(data_test)\n",
    "print (\"KNeighbors accuracy score : \",accuracy_score(target_test, pred))\n",
    "\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "\n",
    "# Instantiate the classification model and visualizer\n",
    "visualizer = ClassificationReport(neigh, classes=['Won','Loss'])\n",
    "\n",
    "visualizer.fit(data_train, target_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(data_test, target_test)  # Evaluate the model on the test data\n",
    "g = visualizer.poof()             # Draw/show/poof the data\n",
    "\n",
    "# KNN\n",
    "from sklearn import neighbors\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # III. Support Vectore Machines\n",
    "# ------------------------------\n",
    "# - SVM - Support Vector Machine\n",
    "\n",
    "\n",
    "# Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear')\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) # Iris-Virginica\n",
    "svm_clf = Pipeline((\n",
    " (\"scaler\", StandardScaler()),\n",
    " (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n",
    " ))\n",
    "svm_clf.fit(X_scaled, y)\n",
    "Then, as usual, you can use the model to make predictions:\n",
    ">>> svm_clf.predict([[5.5, 1.7]])\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "polynomial_svm_clf = Pipeline((\n",
    " (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    " (\"scaler\", StandardScaler()),\n",
    " (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n",
    " ))\n",
    "polynomial_svm_clf.fit(X, y)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "poly_kernel_svm_clf = Pipeline((\n",
    " (\"scaler\", StandardScaler()),\n",
    " (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    " ))\n",
    "poly_kernel_svm_clf.fit(X, y)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import LinearSVR\n",
    "svm_reg = LinearSVR(epsilon=1.5)\n",
    "svm_reg.fit(X, y)\n",
    "\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
    "svm_poly_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IV. Bayesian\n",
    "# ------------\n",
    "# - NB - Naive Bayes\n",
    "# - GBN - Gaussian Naive Bayes\n",
    "# - BBN - Bayesian Belief Network\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(x_training_data, y_training_data)\n",
    "predictions = model.predict(your_x_data)\n",
    "probabilities = model.predict_proba(your_x_data)\n",
    "\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "pred = gnb.fit(data_train, target_train).predict(data_test)\n",
    "#print(pred.tolist())\n",
    "\n",
    "print(\"Naive-Bayes accuracy : \",accuracy_score(target_test, pred, normalize = True))\n",
    "\n",
    "### Performance comparsion\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "\n",
    "# Instantiate the classification model and visualizer\n",
    "visualizer = ClassificationReport(gnb, classes=['Won','Loss'])\n",
    "\n",
    "visualizer.fit(data_train, target_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(data_test, target_test)  # Evaluate the model on the test data\n",
    "g = visualizer.poof()             # Draw/show/poof the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V. Regularisation\n",
    "# -----------------\n",
    "# - Ridge Regression\n",
    "# - LASSO - Least Actual Shrinkage and Sellection Operator\n",
    "# - Elastic Net\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=1, solver=\"cholesky\")\n",
    "ridge_reg.fit(X, y)\n",
    "ridge_reg.predict([[1.5]])\n",
    "#array([[ 1.55071465]])\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X, y)\n",
    "lasso_reg.predict([[1.5]])\n",
    "#array([ 1.53788174])\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "elastic_net.fit(X, y)\n",
    "elastic_net.predict([[1.5]])\n",
    "#array([ 1.54333232])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VI. Rule system\n",
    "# ----------------\n",
    "# - One Rule\n",
    "# - Zero Rule\n",
    "# - Repeated Incremental Pruning to Produce Error Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VII. Decision Trees\n",
    "# -------------------\n",
    "# - CAT - Classification And Regression Trees\n",
    "# - CHAID - Chi-squared Automatic Interaction Detection\n",
    "# - Conditional Decision Tree\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)\n",
    "\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(\n",
    " tree_clf,\n",
    " out_file=image_path(\"iris_tree.dot\"),\n",
    " feature_names=iris.feature_names[2:],\n",
    " class_names=iris.target_names,\n",
    " rounded=True,\n",
    " filled=True\n",
    " )\n",
    "\n",
    "\n",
    "\n",
    "tree_clf.predict_proba([[5, 1.5]])\n",
    "#array([[ 0. , 0.90740741, 0.09259259]])\n",
    "tree_clf.predict([[5, 1.5]])\n",
    "#array([1])\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg.fit(X, y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIII. Ensemble\n",
    "# --------------\n",
    "# - RF - Random Forrest\n",
    "# - Gradient Boosted Machines\n",
    "# - GBRT - Gradient Boostet Regression Trees\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "voting_clf = VotingClassifier(\n",
    " estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    " voting='hard'\n",
    " )\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "#Letâs look at each classifierâs accuracy on the test set:\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "#LogisticRegression 0.864\n",
    "#RandomForestClassifier 0.872\n",
    "#SVC 0.888\n",
    "#VotingClassifier 0.896\n",
    "\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "bag_clf = BaggingClassifier(\n",
    " DecisionTreeClassifier(), n_estimators=500,\n",
    " max_samples=100, bootstrap=True, n_jobs=-1\n",
    " )\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "DecisionTreeClassifier(), n_estimators=500,\n",
    "bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_\n",
    "\n",
    "# 0.93066666666666664\n",
    "# According to this oob evaluation, this BaggingClassifier is likely to achieve about\n",
    "# 93.1% accuracy on the test set. Letâs verify this:\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "#0.93600000000000005\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "The following BaggingClassifier is\n",
    "roughly equivalent to the previous RandomForestClassifier:\n",
    "bag_clf = BaggingClassifier(\n",
    " DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
    " n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1\n",
    " )\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)\n",
    "#sepal length (cm) 0.112492250999\n",
    "\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_clf = AdaBoostClassifier(\n",
    " DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    " algorithm=\"SAMME.R\", learning_rate=0.5\n",
    " )\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X, y)\n",
    "Now train a second DecisionTreeRegressor on the residual errors made by the first\n",
    "predictor:\n",
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X, y2)\n",
    "Then we train a third regressor on the residual errors made by the second predictor:\n",
    "y3 = y2 - tree_reg2.predict(X))\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X, y3)\n",
    "Now we have an ensemble containing three trees. It can make predictions on a new\n",
    "instance simply by adding up the predictions of all the trees:\n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X, y)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "errors = [mean_squared_error(y_val, y_pred)\n",
    " for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors)\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# The following code stops training when the validation error does not\n",
    "# improve for five iterations in a row:\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "    if error_going_up == 5:\n",
    "        break # early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X. Dimensionallity Reduction\n",
    "# ----------------------------\n",
    "# - PCA - Principial Component Analysis\n",
    "# - LDA - Linear Discriminant Analysis\n",
    "# - RDA - Principial Component Regression\n",
    "\n",
    "\n",
    "# Principal Component Analysis (PCA)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "X2D = pca.fit_transform(X)\n",
    " print(pca.explained_variance_ratio_)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components = 154)\n",
    "X_mnist_reduced = pca.fit_transform(X_mnist)\n",
    "X_mnist_recovered = pca.inverse_transform(X_mnist_reduced)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_mnist, n_batches):\n",
    "inc_pca.partial_fit(X_batch)\n",
    "X_mnist_reduced = inc_pca.transform(X_mnist)\n",
    "\n",
    "\n",
    "\n",
    "X_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n))\n",
    "batch_size = m // n_batches\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mm)\n",
    "\n",
    "\n",
    "\n",
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\n",
    "X_reduced = rnd_pca.fit_transform(X_mnist)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.decomposition import KernelPCA\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "clf = Pipeline([\n",
    " (\"kpca\", KernelPCA(n_components=2)),\n",
    " (\"log_reg\", LogisticRegression())\n",
    " ])\n",
    "param_grid = [{\n",
    " \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
    " \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n",
    " }]\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X, y)\n",
    "The best kernel and hyperparameters are then available through the best_params_\n",
    "variable:\n",
    ">>> print(grid_search.best_params_)\n",
    "\n",
    "\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True)\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "X_preimage = rbf_pca.inverse_transform(X_reduced)\n",
    "\n",
    "\n",
    "You can then compute the reconstruction pre-image error:\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(X, X_preimage)\n",
    "32.786308795766132\n",
    "\n",
    "\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
    "X_reduced = lle.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IX. Clustering\n",
    "# --------------\n",
    "# - K-Means\n",
    "# - K-Medians\n",
    "# - Hierarchical Clustering\n",
    "\n",
    "\n",
    "# K Means\n",
    "from sklearn.cluster import KMeans\n",
    "k_means = KMeans(n_clusters=3, random_state=0)\n",
    "\n",
    "# KNMeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model = KMeans(n_clusters=4, init='random')\n",
    "model.fit(x_training_data)\n",
    "predictions = model.predict(your_x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "print(cross_val_score(knn, X_train, y_train, cv=4))\n",
    "print(cross_val_score(lr, X, y, cv=2))\n",
    "\n",
    "# Grid search\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "params = {\"n_neighbors\": np.arange(1,3), \"metric\": [\"euclidean\", \"cityblock\"]}\n",
    "grid = GridSearchCV(estimator=knn, param_grid=params)\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.n_neighbors)\n",
    "\n",
    "# Randomized Parameter Optimization\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "params = {\"n_neighbors\": range(1,5), \"weights\": [\"uniform\", \"distance\"]}\n",
    "rsearch = RandomizedSearchCV(estimator=knn, param_distributions=params, cv=4, n_iter=8, random_state=5)\n",
    "rsearch.fit(X_train, y_train)\n",
    "print(rsearch.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(n_iter=50, penalty=None, eta0=0.1)\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "Once again, you find a solution very close to the one returned by the Normal Equaâ\n",
    "tion:\n",
    ">>> sgd_reg.intercept_, sgd_reg.coef_\n",
    "(array([ 4.18380366]), array([ 2.74205299]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svc_model = LinearSVC(random_state=0)\n",
    "\n",
    "pred = svc_model.fit(data_train, target_train).predict(data_test)\n",
    "\n",
    "print(\"LinearSVC accuracy : \",accuracy_score(target_test, pred, normalize = True))\n",
    "\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "\n",
    "# Instantiate the classification model and visualizer\n",
    "visualizer = ClassificationReport(svc_model, classes=['Won','Loss'])\n",
    "\n",
    "visualizer.fit(data_train, target_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(data_test, target_test)  # Evaluate the model on the test data\n",
    "g = visualizer.poof()             # Draw/show/poof the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "\n",
    "# Unstructured hierarchical clustering\n",
    "st = time.time()\n",
    "ward = AgglomerativeClustering(n_clusters=6, linkage='ward').fit(X)\n",
    "elapsed_time = time.time() - st\n",
    "label = ward.labels_\n",
    "print(\"Elapsed time: %.2fs\" % elapsed_time)\n",
    "print(\"Number of points: %i\" % label.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data generators\n",
    "from sklearn.datasets.samples_generator import make_swiss_roll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    " \n",
    "digits= datasets.load_digits()                     // dataset\n",
    "clf = svm.SVC(gamma=0.001, C=100)\n",
    "print(digits.target)\n",
    "print(digits.images[0])\n",
    "print(len(digits.data))\n",
    "x,y=digits.data[:-1],digits.target[:-1]            // train the data\n",
    "clf.fit(x,y)\n",
    "print('Prediction:', clf.predict(digits.data[-1])) //predict the data\n",
    "plt.imshow(digits.images[-1],cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "plt.show()\n",
    "\n",
    "# OR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    " \n",
    "digits= datasets.load_digits()\n",
    "# Join the images and target labels in a list\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    " \n",
    "# for every element in the list\n",
    "for index, (image, label) in enumerate(images_and_labels[:8]):\n",
    "    # initialize a subplot of 2X4 at the i+1-th position\n",
    "    plt.subplot(2, 4, index + 1)\n",
    "    # Display images in all subplots\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r,interpolation='nearest')\n",
    "    # Add a title to each subplot\n",
    "    plt.title('Training: ' + str(label))\n",
    " \n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# others\n",
    "\n",
    "# Print name\n",
    "for algorithm in [GaussianNB, SVC, DecisionTreeClassifier]:\n",
    "    x = algorithm().fit(X_train, y_train)\n",
    "    print(algorithm.__name__)\n",
    "    print(\"Accuracy\", accuracy_score(y_test, x.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_ / intercept_ / classes_\n",
    "The coef method displays the coefficients of the features in the decision function. The intercept method shows the bias added to the decision function. The classes method outputs the order of the classes.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression().fit(X_train,y_train)\n",
    "accuracy_score(y_test, logreg.predict(X_test))\n",
    "print(\"Coefficients\", logreg.coef_)\n",
    "print(\"Intercept\", logreg.intercept_)\n",
    "print(\"Classes\", logreg.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_\n",
    "Methods that use ensembles of decision trees (like Random Forest or Extra Trees) can also compute the relative importance of each attribute. These importance values can be used to inform a feature selection process.\n",
    "\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "dataset = datasets.load_iris()\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(dataset.data, dataset.target)\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grid Search\n",
    "Grid search is an approach to parameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid. Shown below are methods that help summarize the results of the grid search\n",
    "\n",
    "depth_range = range(1, 10)\n",
    "leaf_range = range(1,15)\n",
    "param_grid = dict(max_depth=depth_range, min_samples_leaf=leaf_range)\n",
    "d_tree = tree.DecisionTreeClassifier()\n",
    "grid = GridSearchCV(d_tree, param_grid, cv=10, scoring='accuracy')\n",
    "grid.fit(X,y)\n",
    "print grid.best_score_\n",
    "print grid.best_params_\n",
    "print grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Rescaling\n",
    "Your preprocessed data may contain attributes with a mixtures of scales for various quantities such as dollars, kilograms and sales volume.\n",
    "\n",
    "Many machine learning methods expect or are more effective if the data attributes have the same scale. Two popular data scaling methods are normalization and standardization. Normalization refers to rescaling real valued numeric attributes into the range 0 and 1. Standardization refers to shifting the distribution of each attribute to have a mean of zero and a standard deviation of one (unit variance).\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import preprocessing\n",
    "iris = load_iris()\n",
    "print(iris.data.shape)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "normalized_X = preprocessing.normalize(X)\n",
    "standardized_X = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas and sklearn's CountVectorizer class\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create a dataframe from a word matrix\n",
    "def wm2df(wm, feat_names):\n",
    "    \n",
    "    # create an index for each row\n",
    "    doc_names = ['Doc{:d}'.format(idx) for idx, _ in enumerate(wm)]\n",
    "    df = pd.DataFrame(data=wm.toarray(), index=doc_names,\n",
    "                      columns=feat_names)\n",
    "    return(df)\n",
    "  \n",
    "# set of documents\n",
    "corpora = ['The quick brown fox.','Jumps over the lazy dog!']\n",
    "\n",
    "# instantiate the vectorizer object\n",
    "cvec = CountVectorizer(lowercase=False)\n",
    "\n",
    "# convert the documents into a document-term matrix\n",
    "wm = cvec.fit_transform(corpora)\n",
    "\n",
    "# retrieve the terms found in the corpora\n",
    "tokens = cvec.get_feature_names()\n",
    "\n",
    "# create a dataframe from the matrix\n",
    "wm2df(wm, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpora = [\n",
    "    'The quick brown fox&#x0002E;',\n",
    "    'jumped over the lazy dog&#x00021;'\n",
    "]\n",
    "\n",
    "cvec = CountVectorizer()\n",
    "wm = cvec.fit_transform(corpora)\n",
    "tokens = cvec.get_feature_names()\n",
    "pd.DataFrame(data=wm.toarray(), index=['Doc1', 'Doc2'],\n",
    "             columns=tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTreeRegressor class and call the fit method:\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor  \n",
    "regressor = DecisionTreeRegressor()  \n",
    "regressor.fit(X_train, y_train)  \n",
    "To make predictions on the test set, ues the predict method:\n",
    "\n",
    "y_pred = regressor.predict(X_test)  \n",
    "Now let's compare some of our predicted values with the actual values and see how accurate we were:\n",
    "\n",
    "df=pd.DataFrame({'Actual':y_test, 'Predicted':y_pred})  \n",
    "df  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generators / test_train / dummy classifier and regressor / cros val / predict / make score / standarisation MiniMAXSaler / Select K Best / Polynomial feature gen / Kernel PCA / GridSearchCV / Custom transformer / preprocessing pipeline / ensemble classification / feature extraction from text / label and one-hot encodersgenerators / test_train / dummy classifier and regressor / cros val / predict / make score / standarisation MiniMAXSaler / Select K Best / Polynomial feature gen / Kernel PCA / GridSearchCV / Custom transformer / preprocessing pipeline / ensemble classification / feature extraction from text / label and one-hot encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =   \n",
    " train_test_split(cancer_data_pd[cancer_data_dict.feature_names],\n",
    "   cancer_data_dict['target'],\n",
    "   test_size=0.20,\n",
    "   stratify=cancer_data_dict['target'],\n",
    "   random_state=111,\n",
    "   shuffle=True);\n",
    "INFO - X_train.shape : (455, 30)\n",
    "INFO - X_test.shape  : (114, 30)\n",
    "INFO - Y_train.shape : (455,)\n",
    "INFO - Y_test.shape  : (114,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DummyRegressors and Classifiers: Before exploratory data analysis and feature selection, I would strongly suggest building a dummy regressor or classifier. DummyClassifier will provide the bias scenario for the model. i.e., In the cancer dataset the majority (most frequent) class is Benign (357 out of 569), so assigning any future test observation (patient) to Benign class will be a dummy classifier. The dummy estimator finds the pattern in the target variable instead of the learning the patterns from the input feature. Why do we need a dummy estimator is to get a baseline for the model performance metrics. Any other machine learning algorithm should at least outperform the dummy estimator\n",
    "\n",
    "dummy_classifier = DummyClassifier(strategy=\"most_frequent\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SelectKBest: Input feature selection is a very critical step in any model building process. The package provides a routine for selecting n best feature based on a given criterion. In the below code the features are selected based on the f_classif criteria (One of the classification model performance metric)\n",
    "selectKbest_est = SelectKBest(f_classif, k=8);\n",
    "selectKbest_X_train = selectKbest_est.fit_transform(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KFold and CrossVal Score/Predict: To avoid the algorithm overfitting to the training set it needs to be generalized to an extent. Instead of running the training algorithm across the entire training set, the training set is split into multiple chunks (i.e., 10 equal pieces) and trained on few chucks (9 pieces used for training) and tested on the rest (1 piece for testing). To avoid overfitting the process will be repeated. An overfit model performs well only with the training set patterns/scenario and fails miserably to make the proper class prediction with the test set. The package provides the KFOLD and CrossVal routines to avoid overfitting. In the below code the kfold is set with 10 splits (10 different groups. Each group will have train input features, train target, test input features, test target). cross_val_score will fit 10 dummy classifier on 10 group of kfold datasets. The accuracy scores will be on a list\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=111);\n",
    "results = model_selection.cross_val_score(dummy_classifier, X_train, y_train, cv=kfold, scoring=accuracy_scorer);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Standardization: It is mandatory to scale all continuous numeric input features so that not a single feature influences the model performance. i.e., input feature A might range in millions and B in hundreds, and if not scaled to a standard scale the model will not learn the variance in feature B. The package comes with minmax (scaled between 0 and 1) and standard scaler (the scale output will include negative values)\n",
    "std_scaler = preprocessing.MinMaxScaler(); \n",
    "std_scaler = std_scaler.fit(X_train);\n",
    "scaled_X_train = pd.DataFrame(std_scaler.transform(X_train), columns=X_train.columns);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Polynomial feature generation: Combining input features to generate polynomial and interaction terms. the packages preprocessing module comes with the polynomial feature routine to generate new features based on the given degree\n",
    "poly = preprocessing.PolynomialFeatures(degree=2, include_bias=False, interaction_only=False);\n",
    "    \n",
    "X_train_poly = poly.fit_transform(X_train);\n",
    "X_train_p2 = pd.DataFrame(X_train_poly, \n",
    "columns=poly.get_feature_names(X_train.columns));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecompositionâââPCA (Kernel PCAâââPrincipal component analysis): This is where the input features are huge in number, and it needs decomposition into a few but preserving the variance across the features. The package comes with KernelPCA routine to condense the features into a smaller set. The method can take various kernels to perform PCA. The data has to scaled for PCA\n",
    "kernel_param = ('rbf', 1);\n",
    "kpca = KernelPCA(n_components=4, \n",
    "                 kernel=kernel_param[0], \n",
    "                 gamma=kernel_param[1], \n",
    "                 fit_inverse_transform=True, \n",
    "                 random_state=111)     \n",
    "kpca.fit(scaled_X_train);   # The data has to be scaled;\n",
    "kpca_X_train = kpca.transform(scaled_X_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grid-search (GridSearchCV): The model parameter tuning is a daunting task, and multiple iterations have to be logged in with their performance metrics until one reaches the best set of parameters. Parameter tuning is mostly simplified in Scikit-learn by the GridSearchCV routine. Given a list of model parameter combinations, the method runs all possible combinations and returns the best model parameter along with the best estimator. The method also performs cross-validation, so the best estimator does not overfit the training data. In the below code there are 8 (2 x 2 x 2 x 1) parameter combinations, and the routine will fit 40 models since the cross-validation is 5\n",
    "tuning_parameters = [{'n_estimators' : [1, 10],\n",
    "                      'max_depth' : [10, 20],\n",
    "                      'max_features' : [0.80, 0.40],\n",
    "                      'random_state' : [111]\n",
    "                     }];\n",
    "clf = GridSearchCV(RandomForestClassifier(), \n",
    "                   tuning_parameters, \n",
    "                   cv=5, \n",
    "                   scoring=accuracy_scorer);\n",
    "clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Custom estimator and Pipeline: Practitioners can code their custom estimators. The custom estimator can be part of a pipeline. A pipeline takes multiple estimators and executes them in sequence. It will pass the output of the previous estimator as the input to the next estimator in the list. An entire model process (standard scaler, imputer, polynomial feature generation, and classification model fit) can be designed using the pipeline, and it can be directly fit to a dataset. This routine is a great help in simplifying the model production deployment. In the below code the ColumnTypeFilter will only return the pandas columns which are of type numpy number. The pipeline takes the output from ColumnTypeFilter and scales them using a standard scaler and min-max scaler. The output will have twice the amount of numeric features as the input\n",
    "class ColumnTypeFilter(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Custom transformer to select all columns of a particular type in a pandas dataframes \"\"\";\n",
    "    def __init__(self, dtype):\n",
    "        self.dtype = dtype;\n",
    "    def fit(self, X, y=None):\n",
    "        return self;\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, pd.DataFrame);\n",
    "        return X.select_dtypes(include=[self.dtype]);\n",
    "ctf = ColumnTypeFilter(np.number);\n",
    "ctf.fit_transform(X_train).head();\n",
    "custom_pipeline = make_pipeline(\n",
    "            FeatureUnion(transformer_list=[\n",
    "                ('StdScl', make_pipeline(\n",
    "                    ColumnTypeFilter(np.number),\n",
    "                    preprocessing.StandardScaler()\n",
    "                )),\n",
    "                ('MMScl', make_pipeline(\n",
    "                    ColumnTypeFilter(np.number),\n",
    "                    preprocessing.MinMaxScaler()\n",
    "                ))\n",
    "            ])\n",
    "    );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ensemble modelsâââVotingClassifier: One of my favorite classification routine. Add as many classifier estimators (estimators should have a predict probability method) into VotingClassifier. For the new test record, the routine will send the record to all estimators and obtain the class predictions and then based on the majority vote it assigns a class\n",
    "Democracy at work.\n",
    "\n",
    "ensemble_clf = VotingClassifier(estimators=[\n",
    "                            ('dummy', dummy_classifier),\n",
    "                            ('logistic', lr),\n",
    "                            ('rf', RandomForestClassifier())],\n",
    "                            voting='soft');\n",
    "ensemble_clf.fit(X_train, y_train);\n",
    "ensemble_clf_accuracy_ = cost_accuracy(y_test,\n",
    "   ensemble_clf.predict(X_test));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Handling categorical and text input features: Any machine learning model will require numeric input features (continuous or categorical) and text features does not integrate well. Scikit-learn has a pill for that too. Use label encoder or one-hot encoder. Below the baby names are converted to numeric vectors, Once turned these vectors will act as the input feature in model training\n",
    "baby_names = ['Ava', 'Lily', 'Noah', 'Jacob', 'Mia', 'Sophia'];\n",
    "X_train_list = [ np.random.choice(baby_names) for i in range(40) ];\n",
    "X_test_list = [ np.random.choice(baby_names) for i in range(6) ];\n",
    "bb_labelencoder = preprocessing.LabelEncoder();\n",
    "bb_labelencoder.fit(X_train_list);\n",
    "bb_encoded = bb_labelencoder.transform(X_test_list);\n",
    "bb_onehotencoder = preprocessing.OneHotEncoder(sparse=False);\n",
    "bb_encoded = bb_encoded.reshape(len(bb_encoded), 1);\n",
    "bb_onehot = bb_onehotencoder.fit_transform(bb_encoded);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature Extraction (From image and text): Use these routines to convert a list of text documents into input feature directly without much code. In the below code the list of sentences is converted to observations with the number of count of the words.\n",
    "corpus = ['This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?', ]\n",
    "vectorizer = CountVectorizer();\n",
    "X = vectorizer.fit_transform(corpus);\n",
    "cntvector_out = pd.DataFrame(X.toarray(), \n",
    "       columns=vectorizer.get_feature_names());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://scikit-learn.org/stable/supervised_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jebanie wykresÃ³w https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.testing import all_estimators\n",
    "\n",
    "estimators = all_estimators()\n",
    "\n",
    "for name, class_ in estimators:\n",
    "    if hasattr(class_, 'predict_proba'):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "# load dataset\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "# prepare configuration for cross validation test harness\n",
    "seed = 7\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "\tkfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "\tcv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "\tresults.append(cv_results)\n",
    "\tnames.append(name)\n",
    "\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "\tprint(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn data to numeric \n",
    "import numpy as np\n",
    "\n",
    "X = np.random.random((10,5))\n",
    "y = np.array(['M','M','F','F','M','F','M','M','F','F','F'])\n",
    "X[X < 0.7] = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
