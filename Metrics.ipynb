{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n",
      "2\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "# do y_test[i] match predictions[i] ?\n",
    "\n",
    "y_test      = ['B', 'A', 'A', 'C', 'B'] # Ground truth \n",
    "predictions = ['A', 'A', 'C', 'B', 'B'] # Predictions\n",
    "\n",
    "y_test      = [2, 1, 1, 3, 2] # Ground truth \n",
    "predictions = [1, 1, 3, 2, 2] # Predictions\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score   = accuracy_score(y_test, predictions) # 1/4\n",
    "print('{}'.format(accuracy_score))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "# return the number of correctly classified samples.\n",
    "accuracy_score = accuracy_score(y_test, predictions, normalize=False)\n",
    "print('{}'.format(accuracy_score))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score = accuracy_score(y_test, predictions, sample_weight=[0,1,0,0,1])\n",
    "print('{}'.format(accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0714285714285714\n",
      "0.9554455445544554\n",
      "[-3.5  1. ]\n"
     ]
    }
   ],
   "source": [
    "# R^2\n",
    "\n",
    "# The coefficient of determination. Scores regression function.\n",
    "\n",
    "# - Returns float (or ndarray of floats if multioutput=‘raw_values’)\n",
    "# - Best score: 1.0\n",
    "# - Can be negative\n",
    "# - Multioutput default: “uniform_average”.\n",
    "# - 'A constant model that always predicts the expected value of y, \n",
    "#    disregarding the input features, would get a R^2 score of 0.0.'\n",
    "# - Not symmetric.\n",
    "\n",
    "# multioutput (default=“uniform_average”)\n",
    "#  Defines aggregating of multiple output scores. \n",
    "#  Array-like value defines weights used to average scores. \n",
    "# \n",
    "#  multioutput=‘raw_values’\n",
    "#   Returns a full set of scores in case of multioutput input.\n",
    "#  multioutput=‘uniform_average’\n",
    "#   Scores of all outputs are averaged with uniform weight.\n",
    "#  multioutput=‘variance_weighted’\n",
    "#   Scores of all outputs are averaged, weighted by the variances of each \n",
    "#   individual output.\n",
    "\n",
    "y_test      = [2, 1, 1, 3, 2] # Ground truth \n",
    "predictions = [2, 1, 2, 2, 1] # Predictions\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2score = r2_score(y_test, predictions)  \n",
    "print('{}'.format(r2score))\n",
    "\n",
    "y_test      = [[0.4, 10], [0.5, 11], [0.6, 12]]\n",
    "predictions = [[0.4, 10], [0.5, 11], [0.3, 12]]\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2score = r2_score(y_test, predictions, multioutput='variance_weighted')  \n",
    "print('{}'.format(r2score))\n",
    "\n",
    "y_test      = [[0.4, 10], [0.5, 11], [0.6, 12]]\n",
    "predictions = [[0.4, 10], [0.5, 11], [0.3, 12]]\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2score = r2_score(y_test, predictions, multioutput='raw_values')  \n",
    "print('{}'.format(r2score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "0.4000000000000001\n",
      "0.4\n",
      "[0.5 0.5 0. ]\n"
     ]
    }
   ],
   "source": [
    "# F1 Score\n",
    "\n",
    "y_test      = ['B', 'A', 'A', 'C', 'B'] # Ground truth \n",
    "predictions = ['A', 'A', 'C', 'B', 'B'] # Predictions\n",
    "\n",
    "y_test      = [2, 1, 1, 3, 2] # Ground truth \n",
    "predictions = [1, 1, 3, 2, 2] # Predictions\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score = f1_score(y_test, predictions, average='macro')\n",
    "print('{}'.format(f1_score))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score = f1_score(y_test, predictions, average='micro')  \n",
    "print('{}'.format(f1_score))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score = f1_score(y_test, predictions, average='weighted')  \n",
    "print('{}'.format(f1_score))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score = f1_score(y_test, predictions, average=None)\n",
    "print('{}'.format(f1_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n",
      "14.026666666666667\n",
      "[ 0.38666667 27.66666667]\n",
      "14.026666666666667\n"
     ]
    }
   ],
   "source": [
    "# Mean Squared Error (MSE)\n",
    "\n",
    "# - Mean squared error regression loss\n",
    "# - Returns: loss (non negative float) for each target.\n",
    "# - multioutput (default=“uniform_average”): Aggregating of multiple output values (weights used to average errors)\n",
    "# - a) multioutput=‘raw_values’: Returns a full set of scores in case of multioutput input.\n",
    "# - b) multioutput=‘uniform_average’:  Scores of all outputs are averaged with uniform weight.\n",
    "\n",
    "\n",
    "\n",
    "y_test      = [3.0, -0.5, 2, 7, 3] # Ground truth\n",
    "predictions = [2.5, -1.0, 2, 8, 3] # Predictions\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print('{}'.format(mse))\n",
    "\n",
    "y_test      = [[0.5, 1], [-11, 13], [2, -4]] # Ground truth\n",
    "predictions = [[0.1, 2], [-11, 22], [3, -3]] # Predictions\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, predictions) # uniform average\n",
    "print('{}'.format(mse))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, predictions, multioutput=[0.5, 0.5])\n",
    "print('{}'.format(mse))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, predictions, multioutput='raw_values')\n",
    "print('{}'.format(mse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss : float or ndarray of floats\n",
    "If multioutput is \n",
    "‘raw_values’, then mean absolute error is returned for each output\n",
    "separately. If multioutput is ‘uniform_average’ or an ndarray of weights,\n",
    "then the weighted average of all output errors is returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "5.488888888888888\n",
      "[ 0.46666667  5.33333333 10.66666667]\n",
      "5.488888888888888\n"
     ]
    }
   ],
   "source": [
    "# Mean Absolute Error (MAE)\n",
    "\n",
    "# Mean absolute error regression loss.\n",
    "# - non-negative float\n",
    "# - the best value is 0.0\n",
    "\n",
    "# - multioutput (default=“uniform_average”): Aggregating of multiple output values (weights used to average errors)\n",
    "# - a) multioutput=‘raw_values’: Returns a full set of scores in case of multioutput input.\n",
    "# - b) multioutput=‘uniform_average’:  Scores of all outputs are averaged with uniform weight.\n",
    "\n",
    "y_test      = [3.5, -0.5, 1, 3, 3] # Ground truth\n",
    "predictions = [2.5, -1.0, 1, 4, 5] # Predictions\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print('{}'.format(mae))\n",
    "\n",
    "y_test      = [[0.5, 1, 1], [-11, 13, 43], [2, 43, -4]] # Ground truth\n",
    "predictions = [[0.1, 2, 1], [-11, 22, 12], [3, 37, -3]] # Predictions\n",
    "\n",
    "# Return MAE score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print('{}'.format(mae))\n",
    "\n",
    "# To return the mean absolute error for each output separately. [0.46666667 3.66666667]\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(y_test, predictions, multioutput='raw_values')\n",
    "print('{}'.format(mae))\n",
    "\n",
    "# Weights\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(y_test, predictions, multioutput=[1, 1, 1])\n",
    "print('{}'.format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
