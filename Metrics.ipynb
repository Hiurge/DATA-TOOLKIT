{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "#### Classification:\n",
    "- Confusion Matrix\n",
    "- Accuracy score\n",
    "- Misclassification score\n",
    "- Precision score (Positive Predictive Value, PPV)\n",
    "- Recall score (Sensitivity, True Positive Rate)\n",
    "- F1 score\n",
    "- Classification Report\n",
    "\n",
    "#### Regression:\n",
    "- R2\n",
    "- MSE\n",
    "- Mean Absolute Error (MAE)\n",
    "\n",
    "#### Clustering:\n",
    "- Adjusted Rand Index\n",
    "- Homogeneity\n",
    "- V-measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Returns: array, shape = [n_classes, n_classes]\n",
    "- 0, 1 and 2 classified as 0, 1 or 2\n",
    "- A/B: TN, FP, FN, TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2\n",
       "0  2  0  0\n",
       "1  0  0  1\n",
       "2  1  0  2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Multiclass with no labels\n",
    "y_test = [2, 0, 2, 2, 0, 1]\n",
    "y_pred = [0, 0, 2, 2, 0, 2]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "display(pd.DataFrame(cf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B  C\n",
       "A  2  0  0\n",
       "B  0  0  1\n",
       "C  1  0  2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Multiclass with labels\n",
    "y_test = [\"C\", \"A\", \"C\", \"C\", \"A\", \"B\"]\n",
    "y_pred = [\"A\", \"A\", \"C\", \"C\", \"A\", \"C\"]\n",
    "labels = [\"A\", \"B\", \"C\"]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cf = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "\n",
    "df = pd.DataFrame(cf, index=labels, columns=labels)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive: 3\n",
      "True Negative: 1\n",
      "False Positive: 1\n",
      "False Negative: 1\n"
     ]
    }
   ],
   "source": [
    "# Extracted binary confusion matrix:\n",
    "y_test = [1, 1, 0, 0,   1, 1]\n",
    "y_pred = [1, 0, 1, 0,   1, 1]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "print('True Positive: {}'.format(tp))\n",
    "print('True Negative: {}'.format(tn))\n",
    "print('False Positive: {}'.format(fp))\n",
    "print('False Negative: {}'.format(fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy score\n",
    " Overall, how often is the classifier correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Accuracy - a ratio of correctly predicted labels to the number of total samples. \n",
    "- Accuracy = TP+TN/TP+FP+FN+TN\n",
    "- Good measure if values of false positive and false negatives are simillar. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "y_test      = ['B', 'A', 'A', 'C', 'B'] # Ground truth \n",
    "predictions = ['A', 'A', 'C', 'B', 'B'] # Predictions\n",
    "\n",
    "# Ratio\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc  = accuracy_score(y_test, predictions) # 2/5\n",
    "print('{}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "y_test      = [2, 1, 1, 3, 2] # Ground truth \n",
    "predictions = [1, 1, 3, 2, 2] # Predictions\n",
    "\n",
    "# Nr of correctly classified samples.\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, predictions, normalize=False)\n",
    "print('{}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "y_test      = [2, 1, 1, 3, 2] # Ground truth \n",
    "predictions = [1, 1, 3, 2, 2] # Predictions\n",
    "\n",
    "# Weighted.\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, predictions, sample_weight=[0,1,0,0,1])\n",
    "print('{}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Misclassification score (1 minus Accuracy)\n",
    "Overall, how often is it wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Misclassification \n",
    "- (FP+FN)/total\n",
    "- equivalent to 1 minus Accuracy\n",
    "- also known as \"Error Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision score (Positive Predictive Value, PPV)\n",
    "\n",
    "When it predicts yes, how often is it correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Returns: float (if average is not None) or array of floats.\n",
    "- Ratio of correctly predicted positive labels to all samples predicted positive.\n",
    "- Of all emails classified as spam, how many actually was a spam?\n",
    "- Precision = TP/TP+FP\n",
    "- High FP = low Precision.\n",
    "- Ability of the classifier not to label as positive a sample that is negative.\n",
    "- The best value is 1 and the worst value is 0.\n",
    "\n",
    "Averaging:\n",
    "-  required for multiclass/multilabel targets. \n",
    "-  None: the scores for each class are returned. \n",
    "- 'binary': Only report results for the class specified by pos_label. \n",
    "- 'micro': calc globally by counting the total true positives, false negatives and false positives.\n",
    "- 'macro': calc each label, and find their unweighted mean. \n",
    "- 'weighted' calc each label, and find their average weighted \n",
    "- 'samples': calc each instance, and find their average.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_true, y_pred, \n",
    "                labels=None, # set of labels to include if not binary\n",
    "                pos_label=1, # class to report if average and data are binary.\n",
    "                average='binary', # below\n",
    "                sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "y_test = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 2, 1, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66666667 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Precision\n",
    "ps = precision_score(y_test, y_pred, average=None)\n",
    "print('{}'.format(ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2222222222222222\n"
     ]
    }
   ],
   "source": [
    "# Precision with macro averaging:\n",
    "ps = precision_score(y_test, y_pred, average='macro') \n",
    "print('{}'.format(ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Precision with micro averaging:\n",
    "ps = precision_score(y_test, y_pred, average='micro')  \n",
    "print('{}'.format(ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2222222222222222\n"
     ]
    }
   ],
   "source": [
    "# Precision with weighted averaging:\n",
    "ps = precision_score(y_test, y_pred, average='weighted')\n",
    "print('{}'.format(ps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall score (Sensitivity, True Positive Rate)\n",
    "When it's actually yes, how often does it predict yes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ability of the classifier to find all the positive samples.\n",
    "- Ratio of correctly predicted positive labels to the all samples in actual class. \n",
    "- Recall = TP/TP+FN\n",
    "- The best value is 1 and the worst value is 0.\n",
    "- Returns: float (if average is not None) or array of floats.\n",
    "\n",
    "Averaging:\n",
    "-  required for multiclass/multilabel targets. \n",
    "-  None: the scores for each class are returned. \n",
    "- 'binary': Only report results for the class specified by pos_label. \n",
    "- 'micro': calc globally by counting the total true positives, false negatives and false positives.\n",
    "- 'macro': calc each label, and find their unweighted mean. \n",
    "- 'weighted' calc each label, and find their average weighted \n",
    "- 'samples': calc each instance, and find their average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_true, y_pred, \n",
    "             labels=None, # set of labels to include if not binary\n",
    "             pos_label=1, # class to report if average and data are binary.\n",
    "             average='binary', # below\n",
    "             sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "y_test = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 2, 1, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Recall score\n",
    "rs = recall_score(y_test, y_pred, average=None)\n",
    "print('{}'.format(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Macro averaged recall score \n",
    "rs = recall_score(y_test, y_pred, average='macro')  \n",
    "print('{}'.format(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Micro averaged recall score \n",
    "rs = recall_score(y_test, y_pred, average='micro')  \n",
    "print('{}'.format(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Weighted averaged recall score \n",
    "rs = recall_score(y_test, y_pred, average='weighted')  \n",
    "print('{}'.format(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Balance between the precision and the recall.\n",
    "- The weighted average of Precision and Recall. \n",
    "- F1 Score = 2*(Recall * Precision) / (Recall + Precision)\n",
    "- Returns: float or array of float, shape = [n_unique_labels]\n",
    "\n",
    "Averaging:\n",
    "-  required for multiclass/multilabel targets. \n",
    "-  None: the scores for each class are returned. \n",
    "- 'binary': Only report results for the class specified by pos_label. \n",
    "- 'micro': calc globally by counting the total true positives, false negatives and false positives.\n",
    "- 'macro': calc each label, and find their unweighted mean. \n",
    "- 'weighted' calc each label, and find their average weighted \n",
    "- 'samples': calc each instance, and find their average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_true, y_pred, \n",
    "         labels=None, # set of labels to include if not binary\n",
    "         pos_label=1, # class to report if average and data are binary.\n",
    "         average='binary', # below\n",
    "         sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test      = ['B', 'A', 'A', 'C', 'B'] # Ground truth \n",
    "predictions = ['A', 'A', 'C', 'B', 'B'] # Predictions\n",
    "\n",
    "y_test      = [2, 1, 1, 3, 2] # Ground truth \n",
    "predictions = [1, 1, 3, 2, 2] # Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5 0.5 0. ]\n"
     ]
    }
   ],
   "source": [
    "# F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score = f1_score(y_test, predictions, average=None)\n",
    "print('{}'.format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Macro averaged F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score = f1_score(y_test, predictions, average='macro')\n",
    "print('{}'.format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4000000000000001\n"
     ]
    }
   ],
   "source": [
    "# Micro averaged F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score = f1_score(y_test, predictions, average='micro')  \n",
    "print('{}'.format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "# Weighted averaged F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score = f1_score(y_test, predictions, average='weighted')  \n",
    "print('{}'.format(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text summary of the precision, recall, F1 score for each class.\n",
    "\n",
    "- Build & show main classification metrics report\n",
    "- Returns string/dict\n",
    "\n",
    "{'label 1': {'precision':0.5,\n",
    "             'recall':1.0,\n",
    "             'f1-score':0.67,\n",
    "             'support':1},\n",
    " 'label 2': { ... },\n",
    "  ...\n",
    "}\n",
    "\n",
    "The reported averages include\n",
    "- micro average (averaging the total true positives, false negatives and false positives), \n",
    "- macro average (averaging the unweighted mean per label), \n",
    "- weighted average (averaging the support-weighted mean per label),\n",
    "- sample average (only for multilabel classification).\n",
    "- recall of the positive is also known as “sensitivity”; \n",
    "- recall of the negativeclass is “specificity”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(y_test, y_pred, \n",
    "                      labels=None, # include list of labels in report\n",
    "                      target_names=None, # display names for labels\n",
    "                      sample_weight=None, # weights for samples\n",
    "                      digits=2, # round output (ignored if dict)\n",
    "                      output_dict=False) #If True: return dict(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Amber       0.50      1.00      0.67         1\n",
      "        Blue       0.00      0.00      0.00         1\n",
      "       Cedar       1.00      0.67      0.80         3\n",
      "\n",
      "   micro avg       0.60      0.60      0.60         5\n",
      "   macro avg       0.50      0.56      0.49         5\n",
      "weighted avg       0.70      0.60      0.61         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_test = [0, 1, 2, 2, 2]\n",
    "y_pred = [0, 0, 2, 2, 1]\n",
    "target_names = ['Amber', 'Blue', 'Cedar']\n",
    "\n",
    "cr = classification_report(y_test, y_pred, target_names=target_names)\n",
    "\n",
    "print(cr)\n",
    "# 0 class - Amber\n",
    "# 1 class - Blue\n",
    "# 2 class - Cedar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#False Positive Rate: When it's actually no, how often does it predict yes?\n",
    "#FP/actual no = 10/60 = 0.17\n",
    "\n",
    "#True Negative Rate: When it's actually no, how often does it predict no?\n",
    "#TN/actual no = 50/60 = 0.83\n",
    "#equivalent to 1 minus False Positive Rate\n",
    "#also known as \"Specificity\"\n",
    "\n",
    "#Prevalence: How often does the yes condition actually occur in our sample?\n",
    "#actual yes/total = 105/165 = 0.64\n",
    "\n",
    "https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/\n",
    "http://cs229.stanford.edu/section/evaluation_metrics.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R^2\n",
    "\n",
    "The coefficient of determination. Scores regression function.\n",
    "\n",
    "- Returns float (or ndarray of floats if multioutput=‘raw_values’)\n",
    "- Best score: 1.0\n",
    "- Can be negative\n",
    "- 'A constant model that always predicts the expected value of y, \n",
    "   disregarding the input features, would get a R^2 score of 0.0.'\n",
    "- Not symmetric.\n",
    "\n",
    "<b>Multioutput</b>. Defines aggregating of multiple output scores, array-like value defines weights used to average scores. (default=“uniform_average”)\n",
    "\n",
    "- multioutput=‘raw_values’: Returns a full set of scores in case of multioutput input.\n",
    "- multioutput=‘uniform_average’: Scores of all outputs are averaged with uniform weight.\n",
    "- multioutput=‘variance_weighted’: Scores of all outputs are averaged, weighted by the variances of each individual output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0714285714285714\n",
      "0.9554455445544554\n",
      "[-3.5  1. ]\n"
     ]
    }
   ],
   "source": [
    "y_test      = [2, 1, 1, 3, 2] # Ground truth \n",
    "predictions = [2, 1, 2, 2, 1] # Predictions\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2score = r2_score(y_test, predictions)  \n",
    "print('{}'.format(r2score))\n",
    "\n",
    "y_test      = [[0.4, 10], [0.5, 11], [0.6, 12]]\n",
    "predictions = [[0.4, 10], [0.5, 11], [0.3, 12]]\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2score = r2_score(y_test, predictions, multioutput='variance_weighted')  \n",
    "print('{}'.format(r2score))\n",
    "\n",
    "y_test      = [[0.4, 10], [0.5, 11], [0.6, 12]]\n",
    "predictions = [[0.4, 10], [0.5, 11], [0.3, 12]]\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2score = r2_score(y_test, predictions, multioutput='raw_values')  \n",
    "print('{}'.format(r2score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "- Mean squared error regression loss\n",
    "- Returns: loss (non negative float) for each target.\n",
    "- multioutput: Aggregating of multiple output values (weights used to average errors)  (default=“uniform_average”):\n",
    "\n",
    "   a) multioutput=‘uniform_average’:  Scores of all outputs are averaged with uniform weight.\n",
    "\n",
    "   b) multioutput=‘raw_values’: Returns a full set of scores in case of multioutput input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n",
      "14.026666666666667\n",
      "14.026666666666667\n",
      "[ 0.38666667 27.66666667]\n"
     ]
    }
   ],
   "source": [
    "y_test      = [3.0, -0.5, 2, 7, 3] # Ground truth\n",
    "predictions = [2.5, -1.0, 2, 8, 3] # Predictions\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print('{}'.format(mse))\n",
    "\n",
    "y_test      = [[0.5, 1], [-11, 13], [2, -4]] # Ground truth\n",
    "predictions = [[0.1, 2], [-11, 22], [3, -3]] # Predictions\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, predictions) # uniform average\n",
    "print('{}'.format(mse))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, predictions, multioutput=[0.5, 0.5])\n",
    "print('{}'.format(mse))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, predictions, multioutput='raw_values')\n",
    "print('{}'.format(mse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Absolute Error (MAE)\n",
    "\n",
    "Mean absolute error regression loss.\n",
    "- non-negative float\n",
    "- the best value is 0.0\n",
    "\n",
    "- multioutput (default=“uniform_average”): Aggregating of multiple output values (weights used to average errors)\n",
    "- a) multioutput=‘raw_values’: Returns a full set of scores in case of multioutput input.\n",
    "- b) multioutput=‘uniform_average’:  Scores of all outputs are averaged with uniform weight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss : float or ndarray of floats\n",
    "If multioutput is \n",
    "‘raw_values’, then mean absolute error is returned for each output\n",
    "separately. If multioutput is ‘uniform_average’ or an ndarray of weights,\n",
    "then the weighted average of all output errors is returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "5.488888888888888\n",
      "[ 0.46666667  5.33333333 10.66666667]\n",
      "5.488888888888888\n"
     ]
    }
   ],
   "source": [
    "y_test      = [3.5, -0.5, 1, 3, 3] # Ground truth\n",
    "predictions = [2.5, -1.0, 1, 4, 5] # Predictions\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print('{}'.format(mae))\n",
    "\n",
    "y_test      = [[0.5, 1, 1], [-11, 13, 43], [2, 43, -4]] # Ground truth\n",
    "predictions = [[0.1, 2, 1], [-11, 22, 12], [3, 37, -3]] # Predictions\n",
    "\n",
    "# Return MAE score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print('{}'.format(mae))\n",
    "\n",
    "# To return the mean absolute error for each output separately. [0.46666667 3.66666667]\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(y_test, predictions, multioutput='raw_values')\n",
    "print('{}'.format(mae))\n",
    "\n",
    "# Weights\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(y_test, predictions, multioutput=[1, 1, 1])\n",
    "print('{}'.format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusted Rand Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rand index adjusted for chance.\n",
    "\n",
    "The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.\n",
    "\n",
    "The raw RI score is then “adjusted for chance” into the ARI score using the following scheme:\n",
    "\n",
    "ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)\n",
    "The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the number of clusters and samples and exactly 1.0 when the clusterings are identical (up to a permutation).\n",
    "\n",
    "ARI is a symmetric measure:\n",
    "\n",
    "adjusted_rand_score(a, b) == adjusted_rand_score(b, a)\n",
    "Read more in the User Guide.\n",
    "\n",
    "Parameters:\t\n",
    "labels_true : int array, shape = [n_samples]\n",
    "Ground truth class labels to be used as a reference\n",
    "\n",
    "labels_pred : array, shape = [n_samples]\n",
    "Cluster labels to evaluate\n",
    "\n",
    "Returns:\t\n",
    "ari : float\n",
    "Similarity score between -1.0 and 1.0. Random labelings have an ARI close to 0.0. 1.0 stands for perfect match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.adjusted_rand_score(labels_true, labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Perfectly matching labelings have a score of 1 even\n",
    "\n",
    ">>>\n",
    ">>> from sklearn.metrics.cluster import adjusted_rand_score\n",
    ">>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
    "1.0\n",
    ">>> adjusted_rand_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
    "1.0\n",
    "Labelings that assign all classes members to the same clusters are complete be not always pure, hence penalized:\n",
    "\n",
    ">>>\n",
    ">>> adjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1])  \n",
    "0.57...\n",
    "ARI is symmetric, so labelings that have pure clusters with members coming from the same classes but unnecessary splits are penalized:\n",
    "\n",
    ">>>\n",
    ">>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 2])  \n",
    "0.57...\n",
    "If classes members are completely split across different clusters, the assignment is totally incomplete, hence the ARI is very low:\n",
    "\n",
    ">>>\n",
    ">>> adjusted_rand_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
    "0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homogeneity Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homogeneity metric of a cluster labeling given a ground truth.\n",
    "\n",
    "A clustering result satisfies homogeneity if all of its clusters contain only data points which are members of a single class.\n",
    "\n",
    "This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won’t change the score value in any way.\n",
    "\n",
    "This metric is not symmetric: switching label_true with label_pred will return the completeness_score which will be different in general.\n",
    "\n",
    "Read more in the User Guide.\n",
    "\n",
    "Parameters:\t\n",
    "labels_true : int array, shape = [n_samples]\n",
    "ground truth class labels to be used as a reference\n",
    "\n",
    "labels_pred : array, shape = [n_samples]\n",
    "cluster labels to evaluate\n",
    "\n",
    "Returns:\t\n",
    "homogeneity : float\n",
    "score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Perfect labelings are homogeneous:\n",
    ">>> from sklearn.metrics.cluster import homogeneity_score\n",
    ">>> homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
    "1.0\n",
    "Non-perfect labelings that further split classes into more clusters can be perfectly homogeneous:\n",
    "\n",
    ">>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))\n",
    "...                                                  \n",
    "1.000000\n",
    ">>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]))\n",
    "...                                                  \n",
    "1.000000\n",
    "Clusters that include samples from different classes do not make for an homogeneous labeling:\n",
    "\n",
    ">>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]))\n",
    "...                                                  \n",
    "0.0...\n",
    ">>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
    "...                                                  \n",
    "0.0..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.homogeneity_score(labels_true, labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V Measure Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V-measure cluster labeling given a ground truth.\n",
    "\n",
    "This score is identical to normalized_mutual_info_score with the 'arithmetic' option for averaging.\n",
    "\n",
    "The V-measure is the harmonic mean between homogeneity and completeness:\n",
    "\n",
    "v = 2 * (homogeneity * completeness) / (homogeneity + completeness)\n",
    "This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won’t change the score value in any way.\n",
    "\n",
    "This metric is furthermore symmetric: switching label_true with label_pred will return the same score value. This can be useful to measure the agreement of two independent label assignments strategies on the same dataset when the real ground truth is not known.\n",
    "\n",
    "Read more in the User Guide.\n",
    "\n",
    "Parameters:\t\n",
    "labels_true : int array, shape = [n_samples]\n",
    "ground truth class labels to be used as a reference\n",
    "\n",
    "labels_pred : array, shape = [n_samples]\n",
    "cluster labels to evaluate\n",
    "\n",
    "Returns:\t\n",
    "v_measure : float\n",
    "score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import v_measure_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Perfect labelings are both homogeneous and complete, hence have score 1.0:\n",
    "\n",
    ">>>\n",
    ">>> from sklearn.metrics.cluster import v_measure_score\n",
    ">>> v_measure_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
    "1.0\n",
    ">>> v_measure_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
    "1.0\n",
    "Labelings that assign all classes members to the same clusters are complete be not homogeneous, hence penalized:\n",
    "\n",
    ">>>\n",
    ">>> print(\"%.6f\" % v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))\n",
    "...                                                  \n",
    "0.8...\n",
    ">>> print(\"%.6f\" % v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))\n",
    "...                                                  \n",
    "0.66...\n",
    "Labelings that have pure clusters with members coming from the same classes are homogeneous but un-necessary splits harms completeness and thus penalize V-measure as well:\n",
    "\n",
    ">>>\n",
    ">>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 1, 2]))\n",
    "...                                                  \n",
    "0.8...\n",
    ">>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 1, 2, 3]))\n",
    "...                                                  \n",
    "0.66...\n",
    "If classes members are completely split across different clusters, the assignment is totally incomplete, hence the V-Measure is null:\n",
    "\n",
    ">>>\n",
    ">>> print(\"%.6f\" % v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))\n",
    "...                                                  \n",
    "0.0...\n",
    "Clusters that include samples from totally different classes totally destroy the homogeneity of the labeling, hence:\n",
    "\n",
    ">>>\n",
    ">>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
    "...                                                  \n",
    "0.0..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completeness metric of a cluster labeling given a ground truth.\n",
    "\n",
    "A clustering result satisfies completeness if all the data points that are members of a given class are elements of the same cluster.\n",
    "\n",
    "This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won’t change the score value in any way.\n",
    "\n",
    "This metric is not symmetric: switching label_true with label_pred will return the homogeneity_score which will be different in general.\n",
    "\n",
    "Read more in the User Guide.\n",
    "\n",
    "Parameters:\t\n",
    "labels_true : int array, shape = [n_samples]\n",
    "ground truth class labels to be used as a reference\n",
    "\n",
    "labels_pred : array, shape = [n_samples]\n",
    "cluster labels to evaluate\n",
    "\n",
    "Returns:\t\n",
    "completeness : float\n",
    "score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.completeness_score(labels_true, labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Perfect labelings are complete:\n",
    "\n",
    ">>>\n",
    ">>> from sklearn.metrics.cluster import completeness_score\n",
    ">>> completeness_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
    "1.0\n",
    "Non-perfect labelings that assign all classes members to the same clusters are still complete:\n",
    "\n",
    ">>>\n",
    ">>> print(completeness_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
    "1.0\n",
    ">>> print(completeness_score([0, 1, 2, 3], [0, 0, 1, 1]))\n",
    "0.999...\n",
    "If classes members are split across different clusters, the assignment cannot be complete:\n",
    "\n",
    ">>>\n",
    ">>> print(completeness_score([0, 0, 1, 1], [0, 1, 0, 1]))\n",
    "0.0\n",
    ">>> print(completeness_score([0, 0, 0, 0], [0, 1, 2, 3]))\n",
    "0.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
