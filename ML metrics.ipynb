{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "#### I. Classification:\n",
    "1. Confusion Matrix\n",
    "2. Accuracy score\n",
    "3. Misclassification score\n",
    "4. Precision score (Positive Predictive Value, PPV)\n",
    "5. Recall score (Sensitivity, True Positive Rate)\n",
    "6. Precision-Recall curve\n",
    "7. F1 score\n",
    "8. Classification Report\n",
    "9. AUC ROC\n",
    "\n",
    "#### II. Regression:\n",
    "1. R2\n",
    "2. MSE\n",
    "3. Mean Absolute Error (MAE)\n",
    "4. Explained Variance\n",
    "\n",
    "#### III. Clustering:\n",
    "1. Adjusted Rand Index\n",
    "2. Homogeneity\n",
    "3. V-measure\n",
    "4. Completeness\n",
    "\n",
    "\n",
    "To be updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Returns: array, shape = [n_classes, n_classes]\n",
    "- 0, 1 and 2 classified as 0, 1 or 2\n",
    "- A/B: TN, FP, FN, TP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass with no labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "y_test = [2, 0, 2, 2, 0, 1]\n",
    "y_pred = [0, 0, 2, 2, 0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2\n",
       "0  2  0  0\n",
       "1  0  0  1\n",
       "2  1  0  2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "display(pd.DataFrame(cf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [\"C\", \"A\", \"C\", \"C\", \"A\", \"B\"]\n",
    "y_pred = [\"A\", \"A\", \"C\", \"C\", \"A\", \"C\"]\n",
    "labels = [\"A\", \"B\", \"C\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B  C\n",
       "A  2  0  0\n",
       "B  0  0  1\n",
       "C  1  0  2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cf = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "\n",
    "df = pd.DataFrame(cf, index=labels, columns=labels)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracted binary confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [1, 1, 0, 0,   1, 1]\n",
    "y_pred = [1, 0, 1, 0,   1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive: 3\n",
      "True Negative: 1\n",
      "False Positive: 1\n",
      "False Negative: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "print('True Positive: {}'.format(tp))\n",
    "print('True Negative: {}'.format(tn))\n",
    "print('False Positive: {}'.format(fp))\n",
    "print('False Negative: {}'.format(fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Accuracy score\n",
    " Overall, how often is the classifier correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Accuracy - a ratio of correctly predicted labels to the number of total samples. \n",
    "- Accuracy = TP+TN/TP+FP+FN+TN\n",
    "- Good measure if values of false positive and false negatives are simillar. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test      = ['B', 'A', 'A', 'C', 'B']\n",
    "predictions = ['A', 'A', 'C', 'B', 'B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc  = accuracy_score(y_test, predictions) # 2/5\n",
    "print('{}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nr of correctly classified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test      = [2, 1, 1, 3, 2]\n",
    "predictions = [1, 1, 3, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, predictions, normalize=False)\n",
    "print('{}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test      = [2, 1, 1, 3, 2]\n",
    "predictions = [1, 1, 3, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, predictions, sample_weight=[0,1,0,0,1])\n",
    "print('{}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Misclassification score (1 minus Accuracy)\n",
    "Overall, how often is it wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Misclassification \n",
    "- (FP+FN)/total\n",
    "- equivalent to 1 minus Accuracy\n",
    "- also known as \"Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:          0.6\n",
      "Misclassification: 0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_test      = [0, 1, 1, 0, 0]\n",
    "predictions = [1, 1, 0, 0, 0]\n",
    "\n",
    "acc = accuracy_score(y_test, predictions)\n",
    "mc  = 1 - acc\n",
    "\n",
    "print('Accuracy:          {}'.format(acc))\n",
    "print('Misclassification: {}'.format(mc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Precision score (Positive Predictive Value, PPV)\n",
    "\n",
    "When it predicts yes, how often is it correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Returns: float (if average is not None) or array of floats.\n",
    "- Ratio of correctly predicted positive labels to all samples predicted positive.\n",
    "- Of all emails classified as spam, how many actually was a spam?\n",
    "- Precision = TP/TP+FP\n",
    "- High FP = low Precision.\n",
    "- Ability of the classifier not to label as positive a sample that is negative.\n",
    "- The best value is 1 and the worst value is 0.\n",
    "\n",
    "Averaging:\n",
    "-  required for multiclass/multilabel targets. \n",
    "-  None: the scores for each class are returned. \n",
    "- 'binary': Only report results for the class specified by pos_label. \n",
    "- 'micro': calc globally by counting the total true positives, false negatives and false positives.\n",
    "- 'macro': calc each label, and find their unweighted mean. \n",
    "- 'weighted' calc each label, and find their average weighted \n",
    "- 'samples': calc each instance, and find their average.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_true, y_pred, \n",
    "                labels=None, # set of labels to include if not binary\n",
    "                pos_label=1, # class to report if average and data are binary.\n",
    "                average='binary', # below\n",
    "                sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "y_test = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 2, 1, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66666667 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "ps = precision_score(y_test, y_pred, average=None)\n",
    "print('{}'.format(ps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision with macro averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2222222222222222\n"
     ]
    }
   ],
   "source": [
    "ps = precision_score(y_test, y_pred, average='macro') \n",
    "print('{}'.format(ps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision with micro averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "ps = precision_score(y_test, y_pred, average='micro')  \n",
    "print('{}'.format(ps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision with weighted averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2222222222222222\n"
     ]
    }
   ],
   "source": [
    "ps = precision_score(y_test, y_pred, average='weighted')\n",
    "print('{}'.format(ps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Precision score\n",
    "Compute average precision (AP) from prediction scores\n",
    "\n",
    "\n",
    "sklearn.metrics.average_precision_score(y_true, y_score, average=’macro’, pos_label=1, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Returns: float\n",
    "- AP summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight:\n",
    "\n",
    "Averaging:\n",
    "-  required for multiclass/multilabel targets. \n",
    "-  None: the scores for each class are returned. \n",
    "- 'binary': Only report results for the class specified by pos_label. \n",
    "- 'micro': calc metrics globally by considering each element of the label indicator matrix as a label.\n",
    "- 'macro': calc each label, and find their unweighted mean. \n",
    "- 'weighted' calc each label, and find their average weighted \n",
    "- 'samples': calc each instance, and find their average.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true   = [0.0, 0.0, 1.00, 1.0]\n",
    "y_scores = [0.1, 0.4, 0.35, 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333333"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "average_precision_score(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Recall score (Sensitivity, True Positive Rate)\n",
    "When it's actually yes, how often does it predict yes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ability of the classifier to find all the positive samples.\n",
    "- Ratio of correctly predicted positive labels to the all samples in actual class. \n",
    "- Recall = TP/TP+FN\n",
    "- The best value is 1 and the worst value is 0.\n",
    "- Returns: float (if average is not None) or array of floats.\n",
    "\n",
    "Averaging:\n",
    "-  required for multiclass/multilabel targets. \n",
    "-  None: the scores for each class are returned. \n",
    "- 'binary': Only report results for the class specified by pos_label. \n",
    "- 'micro': calc globally by counting the total true positives, false negatives and false positives.\n",
    "- 'macro': calc each label, and find their unweighted mean. \n",
    "- 'weighted' calc each label, and find their average weighted \n",
    "- 'samples': calc each instance, and find their average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_true, y_pred, \n",
    "             labels=None, # set of labels to include if not binary\n",
    "             pos_label=1, # class to report if average and data are binary.\n",
    "             average='binary', # below\n",
    "             sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "y_test = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 2, 1, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "rs = recall_score(y_test, y_pred, average=None)\n",
    "print('{}'.format(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Macro averaged recall score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "rs = recall_score(y_test, y_pred, average='macro')  \n",
    "print('{}'.format(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Micro averaged recall score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "rs = recall_score(y_test, y_pred, average='micro')  \n",
    "print('{}'.format(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted averaged recall score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "rs = recall_score(y_test, y_pred, average='weighted')  \n",
    "print('{}'.format(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Precision-Recall curve\n",
    "Compute precision-recall pairs for different probability thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_curve(y_true, probas_pred, pos_label=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The last precision and recall values are 1. and 0. respectively and do not have a corresponding threshold. This ensures that the graph starts on the y axis.\n",
    "- Parameters: targets of binary classification in range {-1, 1} or {0, 1} and estimated probabilities or decision function.\n",
    "\n",
    "Returns:\n",
    "- Precision: array, element i is the precision of predictions with score >= thresholds[i]. Last element is 1.\n",
    "- Recall : array, element i is the recall of predictions with score >= thresholds[i] and the last element is 0.\n",
    "- Thresholds : array, increasing thresholds on the decision function used to compute precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  [1. 1. 1. 1. 1.]\n",
      "Recall:     [1.   0.75 0.5  0.25 0.  ]\n",
      "Thresholds: [0.1 0.4 0.5 0.8]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "y_true   = [1, 1, 1, 1]\n",
    "y_scores = [0.1, 0.4, 0.8, 0.5]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "#(precision, recall, thresholds)\n",
    "print('Precision:  {}'.format(precision))\n",
    "print('Recall:     {}'.format(recall))\n",
    "print('Thresholds: {}'.format(thresholds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_true, y_pred, \n",
    "         labels=None, # set of labels to include if not binary\n",
    "         pos_label=1, # class to report if average and data are binary.\n",
    "         average='binary', # below\n",
    "         sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Balance between the precision and the recall.\n",
    "- The weighted average of Precision and Recall. \n",
    "- F1 Score = 2*(Recall * Precision) / (Recall + Precision)\n",
    "- Returns: float or array of float, shape = [n_unique_labels]\n",
    "\n",
    "Averaging:\n",
    "-  required for multiclass/multilabel targets. \n",
    "-  None: the scores for each class are returned. \n",
    "- 'binary': Only report results for the class specified by pos_label. \n",
    "- 'micro': calc globally by counting the total true positives, false negatives and false positives.\n",
    "- 'macro': calc each label, and find their unweighted mean. \n",
    "- 'weighted' calc each label, and find their average weighted \n",
    "- 'samples': calc each instance, and find their average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test      = ['B', 'A', 'A', 'C', 'B'] \n",
    "predictions = ['A', 'A', 'C', 'B', 'B']\n",
    "\n",
    "y_test      = [2, 1, 1, 3, 2]\n",
    "predictions = [1, 1, 3, 2, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5 0.5 0. ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score = f1_score(y_test, predictions, average=None)\n",
    "print('{}'.format(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Macro averaged F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score = f1_score(y_test, predictions, average='macro')\n",
    "print('{}'.format(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Micro averaged F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4000000000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score = f1_score(y_test, predictions, average='micro')  \n",
    "print('{}'.format(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted averaged F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score = f1_score(y_test, predictions, average='weighted')  \n",
    "print('{}'.format(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(y_test, y_pred, \n",
    "                      labels=None, # include list of labels in report\n",
    "                      target_names=None, # display names for labels\n",
    "                      sample_weight=None, # weights for samples\n",
    "                      digits=2, # round output (ignored if dict)\n",
    "                      output_dict=False) #If True: return dict(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text summary of the precision, recall, F1 score for each class.\n",
    "\n",
    "- Build & show main classification metrics report\n",
    "- Returns string/dict\n",
    "\n",
    "The reported averages include\n",
    "- micro average (averaging the total true positives, false negatives and false positives), \n",
    "- macro average (averaging the unweighted mean per label), \n",
    "- weighted average (averaging the support-weighted mean per label),\n",
    "- sample average (only for multilabel classification).\n",
    "- recall of the positive is also known as “sensitivity”; \n",
    "- recall of the negativeclass is “specificity”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Amber       0.50      1.00      0.67         1\n",
      "        Blue       0.00      0.00      0.00         1\n",
      "       Cedar       1.00      0.67      0.80         3\n",
      "\n",
      "   micro avg       0.60      0.60      0.60         5\n",
      "   macro avg       0.50      0.56      0.49         5\n",
      "weighted avg       0.70      0.60      0.61         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_test = [0, 1, 2, 2, 2]\n",
    "y_pred = [0, 0, 2, 2, 1]\n",
    "target_names = ['Amber', 'Blue', 'Cedar']\n",
    "\n",
    "cr = classification_report(y_test, y_pred, target_names=target_names)\n",
    "\n",
    "print(cr)\n",
    "# 0 class - Amber\n",
    "# 1 class - Blue\n",
    "# 2 class - Cedar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. AUC ROC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve\n",
    "The function roc_curve computes the receiver operating characteristic curve, or ROC curve. Quoting Wikipedia :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve(y_true, y_score, pos_label=None, sample_weight=None, drop_intermediate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sk learn implementation is restricted to the binary classification task.\n",
    "- ROC: Receiver operating characteristic\n",
    "- graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied.\n",
    "- created by plotting fraction of TP out of positives vs. fraction of FP out of the negatives at various threshold settings.\n",
    "- TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate.\n",
    "- Input: requires the true binary value and the target scores, which can either be probability estimates of the positive class, confidence values, or binary decisions\n",
    "\n",
    "Returns:\n",
    "- fpr : array, shape = [>2]: Increasing false positive rates such that element i is the false positive rate of predictions with score >= thresholds[i].\n",
    "- tpr : array, shape = [>2]: Increasing true positive rates such that element i is the true positive rate of predictions with score >= thresholds[i].\n",
    "- thresholds : array, shape = [n_thresholds]: Decreasing thresholds on the decision function used to compute fpr and tpr. thresholds[0] represents no instances being predicted and is arbitrarily set to max(y_score) + 1.\n",
    "\n",
    "Since the thresholds are sorted from low to high values, they are reversed upon returning them to ensure they correspond to both fpr and tpr, which are sorted in reversed order during their calculation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive eate: [0.  0.  0.5 0.5 1. ]\n",
      "True Positive rate:  [0.  0.5 0.5 1.  1. ]\n",
      "Tresholds:           [1.8  0.8  0.4  0.35 0.1 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_test      = [1, 1, 2, 2]\n",
    "predictions = [0.1, 0.4, 0.35, 0.8]\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, predictions, pos_label=2)\n",
    "\n",
    "print('False Positive eate: {}'.format(fpr))\n",
    "print('True Positive rate:  {}'.format(tpr))\n",
    "print('Tresholds:           {}'.format(thresholds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC\n",
    "\n",
    "Compute Area Under the Curve (AUC) using the trapezoidal rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.auc(x, y, reorder='deprecated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this sk learn implementation is restricted to the binary classification task.\n",
    "- This is a general function, given points on a curve. \n",
    "- Parameters:\t\n",
    "- x : array, shape = [n] (monotonic increasing or monotonic decreasing).\n",
    "- y : array, shape = [n] (y coordinates)\n",
    "- Returns: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_test      = [1, 1, 2, 2]\n",
    "predictions = [0.1, 0.4, 0.35, 0.8]\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, predictions, pos_label=2)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "print('AUC: {}'.format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(fpr, tpr, \n",
    "         color='orange',\n",
    "         lw=4, \n",
    "         label='ROC curve (area = %0.2f)' % auc)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \n",
    "         color='blue', \n",
    "         lw=4, \n",
    "         linestyle='--')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "\n",
    "plt.xlabel('False Positive')\n",
    "plt.ylabel('True Positive')\n",
    "plt.title('Receiver Operating Characteristic Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC Score\n",
    "Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_true, y_score, average=’macro’, sample_weight=None, max_fpr=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Returns: auc : float\n",
    "- sk learn implementation is restricted to the binary classification task or multilabel classification task in label indicator format.\n",
    "- Input: y_test, y_score: [n_samples] or [n_samples, n_classes]\n",
    "- Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions.\n",
    "- max_fpr : float > 0 and <= 1, optional: If not None, the standardized partial AUC over the range [0, max_fpr] is returned.\n",
    "- In multi-label classification, the roc_auc_score function is extended by averaging over the labels as above.\n",
    "- Compared to metrics such as the subset accuracy, the Hamming loss, or the F1 score, ROC doesn’t require optimizing a threshold for each label. \n",
    "- The roc_auc_score function can also be used in multi-class classification, if the predicted outputs have been binarized.\n",
    "- In applications where a high false positive rate is not tolerable the parameter max_fpr of roc_auc_score can be used to summarize the ROC curve up to the given limit.\n",
    "\n",
    "Averaging:\n",
    "- If None, the scores for each class are returned.\n",
    "- 'micro': calc globally by each element of the label indicator matrix as a label.\n",
    "- 'macro': calc each label, and find their unweighted mean.\n",
    "- 'weighted' calc each label, and find their average weighted\n",
    "- 'samples': calc each instance, and find their average.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_true = [0, 0, 1, 1]\n",
    "y_scores = [0.1, 0.4, 0.35, 0.8]\n",
    "\n",
    "roc_auc_score(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. R2\n",
    "\n",
    "The coefficient of determination. Scores regression function.\n",
    "\n",
    "- Returns float (or ndarray of floats if multioutput=‘raw_values’)\n",
    "- Best score: 1.0\n",
    "- Can be negative\n",
    "- 'A constant model that always predicts the expected value of y, \n",
    "   disregarding the input features, would get a R^2 score of 0.0.'\n",
    "- Not symmetric.\n",
    "\n",
    "<b>Multioutput</b>. Defines aggregating of multiple output scores, array-like value defines weights used to average scores. (default=“uniform_average”)\n",
    "\n",
    "- multioutput=‘raw_values’: Returns a full set of scores in case of multioutput input.\n",
    "- multioutput=‘uniform_average’: Scores of all outputs are averaged with uniform weight.\n",
    "- multioutput=‘variance_weighted’: Scores of all outputs are averaged, weighted by the variances of each individual output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16666666666666674\n"
     ]
    }
   ],
   "source": [
    "y_test      = [11, 11, 10, 10, 11] # Ground truth \n",
    "predictions = [11, 11, 10, 10, 10] # Predictions\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2score = r2_score(y_test, predictions)  \n",
    "print('{}'.format(r2score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.587378640776699\n"
     ]
    }
   ],
   "source": [
    "y_test      = [[0.4, 11], [0.5, 11], [0.6, 12]]\n",
    "predictions = [[0.4, 10], [0.5, 11], [0.3, 12]]\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2score = r2_score(y_test, predictions, multioutput='variance_weighted')  \n",
    "print('{}'.format(r2score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.5  1. ]\n"
     ]
    }
   ],
   "source": [
    "y_test      = [[0.4, 10], [0.5, 11], [0.6, 12]]\n",
    "predictions = [[0.4, 10], [0.5, 11], [0.3, 12]]\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2score = r2_score(y_test, predictions, multioutput='raw_values')  \n",
    "print('{}'.format(r2score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mean Squared Error (MSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mean squared error regression loss\n",
    "- Returns: loss (non negative float) for each target.\n",
    "- multioutput: Aggregating of multiple output values (weights used to average errors)  (default=“uniform_average”):\n",
    "\n",
    "   a) multioutput=‘uniform_average’:  Scores of all outputs are averaged with uniform weight.\n",
    "\n",
    "   b) multioutput=‘raw_values’: Returns a full set of scores in case of multioutput input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_test      = [3.0, -0.5, 2, 7, 3] # Ground truth\n",
    "predictions = [2.5, -1.0, 2, 8, 3] # Predictions\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "\n",
    "print('{}'.format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.026666666666667\n"
     ]
    }
   ],
   "source": [
    "y_test      = [[0.5, 1], [-11, 13], [2, -4]] # Ground truth\n",
    "predictions = [[0.1, 2], [-11, 22], [3, -3]] # Predictions\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions) # uniform average\n",
    "print('{}'.format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.026666666666667\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test, predictions, multioutput=[0.5, 0.5])\n",
    "print('{}'.format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.38666667 27.66666667]\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test, predictions, multioutput='raw_values')\n",
    "print('{}'.format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Mean Absolute Error (MAE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "  <mtext>MAE</mtext>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mi>y</mi>\n",
    "  <mo>,</mo>\n",
    "  <mover>\n",
    "    <mi>y</mi>\n",
    "    <mo stretchy=\"false\">&#x005E;<!-- ^ --></mo>\n",
    "  </mover>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo>=</mo>\n",
    "  <mfrac>\n",
    "    <mn>1</mn>\n",
    "    <msub>\n",
    "      <mi>n</mi>\n",
    "          <mtext>samples</mtext>\n",
    "    </msub>\n",
    "  </mfrac>\n",
    "  <munderover>\n",
    "    <mo>&#x2211;<!-- ∑ --></mo>\n",
    "    <mrow>\n",
    "      <mi>i</mi>\n",
    "      <mo>=</mo>\n",
    "      <mn>0</mn>\n",
    "    </mrow>\n",
    "    <mrow>\n",
    "      <msub>\n",
    "        <mi>n</mi>\n",
    "              <mtext>samples</mtext>\n",
    "      </msub>\n",
    "      <mo>&#x2212;<!-- − --></mo>\n",
    "      <mn>1</mn>\n",
    "    </mrow>\n",
    "  </munderover>\n",
    "  <mrow>\n",
    "    <mo>|</mo>\n",
    "    <mrow>\n",
    "      <msub>\n",
    "        <mi>y</mi>\n",
    "        <mi>i</mi>\n",
    "      </msub>\n",
    "      <mo>&#x2212;<!-- − --></mo>\n",
    "      <msub>\n",
    "              <mover>\n",
    "          <mi>y</mi>\n",
    "          <mo stretchy=\"false\">&#x005E;<!-- ^ --></mo>\n",
    "        </mover>\n",
    "        <mi>i</mi>\n",
    "      </msub>\n",
    "    </mrow>\n",
    "    <mo>|</mo>\n",
    "  </mrow>\n",
    "  <mo>.</mo>\n",
    "</math>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean absolute error regression loss.\n",
    "- non-negative float\n",
    "- the best value is 0.0\n",
    "- returns float or ndarray of floats\n",
    "- multioutput (default=“uniform_average”): Aggregating of multiple output values (weights used to average errors)\n",
    "- a) multioutput=‘raw_values’: Returns a full set of scores in case of multioutput input.\n",
    "- b) multioutput=‘uniform_average’:  Scores of all outputs are averaged with uniform weight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_test      = [3.5, -0.5, 1, 3, 3] # Ground truth\n",
    "predictions = [2.5, -1.0, 1, 4, 5] # Predictions\n",
    "\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print('{}'.format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return MAE score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.488888888888888\n"
     ]
    }
   ],
   "source": [
    "y_test      = [[0.5, 1, 1], [-11, 13, 43], [2, 43, -4]] # Ground truth\n",
    "predictions = [[0.1, 2, 1], [-11, 22, 12], [3, 37, -3]] # Predictions\n",
    "\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print('{}'.format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To return the mean absolute error for each output separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.46666667  5.33333333 10.66666667]\n"
     ]
    }
   ],
   "source": [
    "mae = mean_absolute_error(y_test, predictions, multioutput='raw_values')\n",
    "print('{}'.format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.488888888888888\n"
     ]
    }
   ],
   "source": [
    "mae = mean_absolute_error(y_test, predictions, multioutput=[1, 1, 1])\n",
    "print('{}'.format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Explained Variance\n",
    "Explained variance regression score function\n",
    "\n",
    "sklearn.metrics.explained_variance_score(y_true, y_pred, sample_weight=None, multioutput='uniform_average')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best possible score is 1.0\n",
    "- Not a symmetric function.\n",
    "- Returns: float or ndarray of floats\n",
    "\n",
    "Multioutput: defines aggregating of multiple output scores.\n",
    "Array-like value defines weights used to average scores.\n",
    "- raw_values: Returns a full set of scores in case of multioutput input.\n",
    "- uniform_average: Scores of all outputs are averaged with uniform weight.\n",
    "- variance_weighted: Scores of all outputs are averaged, weighted by the variances of each individual output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import explained_variance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06144638403990055"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [3, -5.5, 5, 5]\n",
    "y_pred = [4,  5.2, 6, 6]\n",
    "explained_variance_score(y_true, y_pred)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49382716049382713"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[5, -14], [5, -5], [5, -5]]\n",
    "y_pred = [[2, -12], [3, -2], [4, -3]]\n",
    "explained_variance_score(y_true, y_pred, multioutput='uniform_average')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Adjusted Rand Score\n",
    "Rand index adjusted for chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- Returns: float\n",
    "- Similarity score between -1.0 and 1.0. \n",
    "- Random labelings have an ARI close to 0.0. 1.0 stands for perfect match.\n",
    "- ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)\n",
    "- ARI is a symmetric: adjusted_rand_score(a, b) == adjusted_rand_score(b, a)\n",
    "- computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.\n",
    "- The raw RI score is then “adjusted for chance” into the ARI score using the following scheme:\n",
    "- The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the number of clusters and samples and exactly 1.0 when the clusterings are identical (up to a permutation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perfectly matching labelings have a score of 1 even"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_rand_score([0, 1, 1, 1], [0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labelings that assign all classes members to the same clusters are complete be not always pure, hence penalized:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285715"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARI is symmetric, so labelings that have pure clusters with members coming from the same classes but unnecessary splits are penalized:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285715"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If classes members are completely split across different clusters, the assignment is totally incomplete, hence the ARI is very low:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_rand_score([0, 0, 0, 0], [0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Homogeneity Score\n",
    "A clustering result satisfies homogeneity if all of its clusters contain only data points which are members of a single class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Returns: float\n",
    "- score between 0.0 and 1.0. \n",
    "- 1.0 stands for perfectly homogeneous labeling\n",
    "- This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won’t change the score value in any way.\n",
    "- This metric is not symmetric: switching label_true with label_pred will return the completeness_score which will be different in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import homogeneity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perfect labelings are homogeneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clusters that include samples from different classes do not make for an homogeneous labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. V Measure Score\n",
    "The V-measure is the harmonic mean between homogeneity and completeness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Returns: float\n",
    "- score between 0.0 and 1.0.\n",
    "- 1.0 stands for perfectly complete labeling\n",
    "- This score is identical to normalized_mutual_info_score with the 'arithmetic' option for averaging.\n",
    "- v = 2 * (homogeneity * completeness) / (homogeneity + completeness)\n",
    "- This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won’t change the score value in any way.\n",
    "- This metric is symmetric: switching label_true with label_pred will return the same score value.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import v_measure_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perfect labelings are both homogeneous and complete, hence have score 1.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_measure_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
    "v_measure_score([0, 0, 1, 1], [1, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labelings that assign all classes members to the same clusters are complete be not homogeneous, hence penalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.800000\n",
      "0.666667\n"
     ]
    }
   ],
   "source": [
    "print(\"%.6f\" % v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))\n",
    "print(\"%.6f\" % v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labelings that have pure clusters with members coming from the same classes are homogeneous but un-necessary splits harms completeness and thus penalize V-measure as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.800000\n",
      "0.666667\n"
     ]
    }
   ],
   "source": [
    "print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 1, 2]))\n",
    "print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If classes members are completely split across different clusters, the assignment is totally incomplete, hence the V-Measure is null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"%.6f\" % v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clusters that include samples from totally different classes totally destroy the homogeneity of the labeling, hence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 0, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Completeness\n",
    "Completeness metric of a cluster labeling given a ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Returns: completeness : float\n",
    "- score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n",
    "- A clustering result satisfies completeness if all the data points that are members of a given class are elements of the same cluster.\n",
    "- independent of the absolute values of the labels\n",
    "- a permutation of the class or cluster label values won’t change the score value in any way.\n",
    "- not symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perfect labelings: complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.cluster import completeness_score\n",
    "completeness_score([0, 0, 1, 1], [1, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labelings that assign all classes members to the same clusters: complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(completeness_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
    "print(completeness_score([0, 1, 2, 3], [0, 0, 1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes members split across different clusters: not complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(completeness_score([0, 0, 1, 1], [0, 1, 0, 1]))\n",
    "print(completeness_score([0, 0, 0, 0], [0, 1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To do:\n",
    "- More on Loss Functions\n",
    "- Cross-Entropy (log loss) # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html\n",
    "- Hinge https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hinge_loss.html#sklearn.metrics.hinge_loss\n",
    "- Huber\n",
    "- Hammington https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html\n",
    "- Kullback-Leibler\n",
    "- https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics\n",
    "- https://scikit-learn.org/stable/modules/learning_curve.html\n",
    "\n",
    "\n",
    "Resources: https://scikit-learn.org/\n",
    "\n",
    "To be updated.\n",
    "\n",
    "By Luke 23.02.2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See also:\n",
    "\n",
    "#False Positive Rate: When it's actually no, how often does it predict yes?\n",
    "#FP/actual no = 10/60 = 0.17\n",
    "\n",
    "#True Negative Rate: When it's actually no, how often does it predict no?\n",
    "#TN/actual no = 50/60 = 0.83\n",
    "#equivalent to 1 minus False Positive Rate\n",
    "#also known as \"Specificity\"\n",
    "\n",
    "#Prevalence: How often does the yes condition actually occur in our sample?\n",
    "#actual yes/total = 105/165 = 0.64\n",
    "\n",
    "#https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/\n",
    "#http://cs229.stanford.edu/section/evaluation_metrics.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
