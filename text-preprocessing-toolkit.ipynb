{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing toolkit\n",
    "\n",
    "#### Scripts:\n",
    "1. Twitter data preprocessing: hashtags, mentions gathering and removal\n",
    "2. Get and remove URL from text\n",
    "3. HTML decoding\n",
    "4. UTF-8 BOM (Byte Order Mark)\n",
    "5. Remove all special characters from string\n",
    "6. Lowercase all string characters\n",
    "7. Remove string inner spaces (any number of extra spaces)\n",
    "8. Remove stop words (NLTK)\n",
    "9. Tokenize words (NLTK)\n",
    "10. Stem words (Porter, Lancaster) (NLTK)\n",
    "11. Lemmatizing words (NLTK WordNetLemmatizer)\n",
    "12. Vectorize text with Bag of Words (sklearn)\n",
    "\n",
    "\n",
    "(To be re-organised, enriched with examples & developed further.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Twitter data preprocessing: hashtags and mentions \n",
    "# --------------------------\n",
    "# 1. get hashtags list\n",
    "# 2. get mentions list\n",
    "# 3. remove hashtags from text\n",
    "# 4. remove mentions from text\n",
    "# 5. remove hashtags and mentions from text\n",
    "# 6. get hashtags and mentions list and remove them from text (Depending on previous funcs)\n",
    "# 7. point 6 autonomic alternative\n",
    "\n",
    "def get_mentions(text):\n",
    "    return ', '.join([w for w in text.split(' ') if w.startswith('@')])\n",
    "def get_hashtags(text):\n",
    "    return ', '.join([w for w in text.split(' ') if w.startswith('#')])\n",
    "def remove_mentions(text):\n",
    "    return ' '.join([w for w in text.split(' ') if not w.startswith('@')])\n",
    "def remove_hashtags(text):\n",
    "    return ' '.join([w for w in text.split(' ') if not w.startswith('#')])\n",
    "def remove_hashtags_and_mentions(text):\n",
    "    return ' '.join([w for w in text.split(' ') if not w.startswith('#') and not w.startswith('@')])\n",
    "\n",
    "def get_and_remove_hashtags_and_mentions_from_text_DEP(text):\n",
    "    return get_hashtags(text), get_mentions(text), remove_hashtags_and_mentions(text)\n",
    "\n",
    "def get_and_remove_hashtags_and_mentions_from_text_AUTO(text):\n",
    "    hashtags, mentions, cleantxt = [],[],[]\n",
    "    for word in text.split(' '):\n",
    "        if   word.startswith('#'): hashtags.append(word)\n",
    "        elif word.startswith('@'): mentions.append(word)\n",
    "        else:                      cleantxt.append(word)\n",
    "    return ', '.join(hashtags),  ', '.join(mentions),  ' '.join(cleantxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Get and remove URL from text\n",
    "# ------------------------------\n",
    "# a. get url as a string\n",
    "# b. get text without url\n",
    "# c. get url and text without url\n",
    "\n",
    "import re\n",
    "\n",
    "def get_url(text):\n",
    "    # Returns '' or url string\n",
    "    try: \n",
    "        return re.search(\"(?P<url>https?://[^\\s]+)\", text).group(\"url\")\n",
    "    except: \n",
    "        return ''\n",
    "def remove_url(text):\n",
    "    # Returns string without url\n",
    "    return re.sub('https?://[A-Za-z0-9./]+','',text)\n",
    "\n",
    "def get_and_remove_url_from_text(text):\n",
    "    # returns '' or url string and string without url\n",
    "    return get_url(text), remove_url(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. HTML decoding\n",
    "# ----------------\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML decoding (works same)\n",
    "def html_strip_lxml(text):\n",
    "    return BeautifulSoup(text, 'lxml').get_text()\n",
    "def html_strip_praser(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. UTF-8 BOM (Byte Order Mark)\n",
    "# ------------------------------\n",
    "\n",
    "def get_BOM_in_order(text):\n",
    "    try:\n",
    "        return text.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Remove all special characters from string\n",
    "# --------------------------------------------\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(\"[^a-zA-Z]\", \" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Lowercase all string characters\n",
    "# ----------------------------------\n",
    "\n",
    "def lowercase_text(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Remove string inner spaces (any number of extra spaces)\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "def strip_inner_spaces(text):\n",
    "    return ' '.join([w.strip() for w in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Remove stop words (NLTK)\n",
    "# ---------------------------\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords') \n",
    "\n",
    "def remove_stop_words(text):\n",
    "    return ' '.join([w for w in text.split() if not w in set(stopwords.words('english'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Tokenize words (NLTK)\n",
    "# ------------------------\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "def tokenize_words(text):\n",
    "    words = tok.tokenize(text)\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Stem words (Porter, Lancaster) (NLTK)\n",
    "# -----------------------------------------\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "# Porter\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "PS = PorterStemmer()\n",
    "\n",
    "def stem_words_porter(text, PS):\n",
    "    return ' '.join([PS.stem(w) for w in text.split()]) \n",
    "\n",
    "\n",
    "# Lancaster\n",
    "from nltk.stem import LancasterStemmer\n",
    "LS = LancasterStemmer()\n",
    "\n",
    "def stem_words_lancaster(text, LS):\n",
    "    return ' '.join([LS.stem(w) for w in text.split()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Lemmatizing words (NLTK WordNetLemmatizer)\n",
    "# -------------------------------------\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "WNL = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(text, WNL):\n",
    "    return ' '.join([WNL.lemmatize(word, pos='v') for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Vectorize text with Bag of Words (sklearn)\n",
    "# ----------------------------------------------\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "  \n",
    "def vectorize_texts(all_texts_list, max_features=1000):\n",
    "    cv = CountVectorizer(max_features=max_features) # Play free with max_features \n",
    "    vec_matrix = cv.fit_transform(raw_documents=all_texts_list).toarray()\n",
    "    print('Vectorised {} texts into {} features.'.format(vec_matrix.shape[0], vec_matrix.shape[1]))\n",
    "    return vec_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
